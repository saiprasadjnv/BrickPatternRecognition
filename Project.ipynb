{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from skimage.feature import hog\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(images) :\n",
    "    X = np.empty((0,1152),dtype='i2')\n",
    "    for image in images:\n",
    "        fd1, hog_image = hog(image, orientations=8, pixels_per_cell=(16, 16),\n",
    "                    cells_per_block=(1, 1), visualize=True, multichannel=True)\n",
    "        X = np.append(X,np.reshape(fd1,(1,-1)),axis=0)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def denoise_image(images):\n",
    "#     no_noise = []\n",
    "# #     no_noise = np.empty(images.shape)\n",
    "#     for i in range(len(images)):\n",
    "#         blur = cv2.GaussianBlur(images[i], (5, 5), 0)\n",
    "#         no_noise.append(blur)\n",
    "#     return no_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_binary_labels(labels):\n",
    "    bin_y = []\n",
    "    for i in range(0, len(labels)):\n",
    "        if labels[i] != 0:\n",
    "            bin_y.append(1)\n",
    "        else:\n",
    "            bin_y.append(0)\n",
    "    return bin_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_edges(image):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_into_classes(images):\n",
    "    class_0, class_1, class_2, class_3, class_4 = []\n",
    "    return class_0, class_1, class_2, class_3, class_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"Images.npy\")\n",
    "y = np.load(\"Labels.npy\")\n",
    "# images_test = np.load(\"\")\n",
    "# labels_test = np.load(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "bin_y = convert_to_binary_labels(y_train)\n",
    "y_test = convert_to_binary_labels(y_test)\n",
    "print(len(bin_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82, 1152)\n"
     ]
    }
   ],
   "source": [
    "X_train = extract_features(X_train)\n",
    "X_test = extract_features(X_test)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = denoise_image(X_train)\n",
    "# print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train data\n",
    "class trainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "train_data = trainData(torch.FloatTensor(X_train), \n",
    "                       torch.FloatTensor(bin_y))\n",
    "## test data    \n",
    "class testData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "test_data = testData(torch.FloatTensor(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class binaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(binaryClassification, self).__init__()\n",
    "        # Number of input features is 12.\n",
    "        self.layer_1 = nn.Linear(1152, 64) \n",
    "        self.layer_2 = nn.Linear(64, 64)\n",
    "        self.layer_out = nn.Linear(64, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = binaryClassification()\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binaryClassification(\n",
      "  (layer_1): Linear(in_features=1152, out_features=64, bias=True)\n",
      "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = binaryClassification()\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.66596 | Acc: 58.500\n",
      "Epoch 002: | Loss: 0.38526 | Acc: 92.000\n",
      "Epoch 003: | Loss: 0.27406 | Acc: 97.500\n",
      "Epoch 004: | Loss: 0.24679 | Acc: 97.000\n",
      "Epoch 005: | Loss: 0.21823 | Acc: 100.000\n",
      "Epoch 006: | Loss: 0.16334 | Acc: 99.000\n",
      "Epoch 007: | Loss: 0.14499 | Acc: 100.000\n",
      "Epoch 008: | Loss: 0.15924 | Acc: 100.000\n",
      "Epoch 009: | Loss: 0.18924 | Acc: 100.000\n",
      "Epoch 010: | Loss: 0.11884 | Acc: 100.000\n",
      "Epoch 011: | Loss: 0.10942 | Acc: 100.000\n",
      "Epoch 012: | Loss: 0.11100 | Acc: 100.000\n",
      "Epoch 013: | Loss: 0.13194 | Acc: 100.000\n",
      "Epoch 014: | Loss: 0.10758 | Acc: 100.000\n",
      "Epoch 015: | Loss: 0.08935 | Acc: 100.000\n",
      "Epoch 016: | Loss: 0.09440 | Acc: 97.000\n",
      "Epoch 017: | Loss: 0.07995 | Acc: 100.000\n",
      "Epoch 018: | Loss: 0.06046 | Acc: 100.000\n",
      "Epoch 019: | Loss: 0.09199 | Acc: 100.000\n",
      "Epoch 020: | Loss: 0.05407 | Acc: 100.000\n",
      "Epoch 021: | Loss: 0.06018 | Acc: 100.000\n",
      "Epoch 022: | Loss: 0.05514 | Acc: 100.000\n",
      "Epoch 023: | Loss: 0.13005 | Acc: 97.000\n",
      "Epoch 024: | Loss: 0.05551 | Acc: 100.000\n",
      "Epoch 025: | Loss: 0.03739 | Acc: 100.000\n",
      "Epoch 026: | Loss: 0.04080 | Acc: 100.000\n",
      "Epoch 027: | Loss: 0.03370 | Acc: 100.000\n",
      "Epoch 028: | Loss: 0.04222 | Acc: 100.000\n",
      "Epoch 029: | Loss: 0.04204 | Acc: 100.000\n",
      "Epoch 030: | Loss: 0.03466 | Acc: 100.000\n",
      "Epoch 031: | Loss: 0.02394 | Acc: 100.000\n",
      "Epoch 032: | Loss: 0.03316 | Acc: 100.000\n",
      "Epoch 033: | Loss: 0.03208 | Acc: 100.000\n",
      "Epoch 034: | Loss: 0.02156 | Acc: 100.000\n",
      "Epoch 035: | Loss: 0.02767 | Acc: 100.000\n",
      "Epoch 036: | Loss: 0.02956 | Acc: 100.000\n",
      "Epoch 037: | Loss: 0.02578 | Acc: 100.000\n",
      "Epoch 038: | Loss: 0.02282 | Acc: 100.000\n",
      "Epoch 039: | Loss: 0.06455 | Acc: 100.000\n",
      "Epoch 040: | Loss: 0.03981 | Acc: 100.000\n",
      "Epoch 041: | Loss: 0.01637 | Acc: 100.000\n",
      "Epoch 042: | Loss: 0.01507 | Acc: 100.000\n",
      "Epoch 043: | Loss: 0.02026 | Acc: 100.000\n",
      "Epoch 044: | Loss: 0.01950 | Acc: 100.000\n",
      "Epoch 045: | Loss: 0.01473 | Acc: 100.000\n",
      "Epoch 046: | Loss: 0.01282 | Acc: 100.000\n",
      "Epoch 047: | Loss: 0.04157 | Acc: 97.000\n",
      "Epoch 048: | Loss: 0.01211 | Acc: 100.000\n",
      "Epoch 049: | Loss: 0.01414 | Acc: 100.000\n",
      "Epoch 050: | Loss: 0.01414 | Acc: 100.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "#         print(X_batch[0])\n",
    "#         print(y_batch.shape)\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3749, 0.2593, 0.4992,  ..., 0.3244, 0.2002, 0.2939])\n",
      "tensor([0.3856, 0.1389, 0.3071,  ..., 0.3499, 0.1877, 0.1023])\n",
      "tensor([0.4347, 0.4347, 0.2425,  ..., 0.1606, 0.2639, 0.4493])\n",
      "tensor([0.3842, 0.2665, 0.3395,  ..., 0.3114, 0.2877, 0.3382])\n",
      "tensor([0.4044, 0.3643, 0.2719,  ..., 0.2992, 0.1385, 0.4943])\n",
      "tensor([0.4344, 0.4344, 0.4344,  ..., 0.3536, 0.3536, 0.3536])\n",
      "tensor([0.3726, 0.1184, 0.2947,  ..., 0.2590, 0.2749, 0.3101])\n",
      "tensor([0.3814, 0.3657, 0.2690,  ..., 0.4550, 0.4550, 0.1731])\n",
      "tensor([0.3986, 0.3454, 0.3986,  ..., 0.2596, 0.3822, 0.3822])\n",
      "tensor([0.0220, 0.0000, 0.0558,  ..., 0.2970, 0.3544, 0.2161])\n",
      "tensor([0.4767, 0.4767, 0.4767,  ..., 0.3536, 0.3536, 0.3536])\n",
      "tensor([0.3536, 0.3536, 0.3536,  ..., 0.0700, 0.2139, 0.3486])\n",
      "tensor([0.1155, 0.0456, 0.2108,  ..., 0.3410, 0.2895, 0.1740])\n",
      "tensor([0.4153, 0.2826, 0.4153,  ..., 0.3573, 0.2130, 0.3166])\n",
      "tensor([0.3536, 0.3536, 0.3536,  ..., 0.2873, 0.4662, 0.2557])\n",
      "tensor([0.1659, 0.0809, 0.1757,  ..., 0.5140, 0.2555, 0.1167])\n",
      "tensor([0.3915, 0.3915, 0.3915,  ..., 0.2624, 0.3232, 0.1505])\n",
      "tensor([0.5207, 0.5207, 0.2535,  ..., 0.0677, 0.1630, 0.6457])\n",
      "tensor([0.1471, 0.0790, 0.2008,  ..., 0.4699, 0.2262, 0.1937])\n",
      "tensor([0.5063, 0.1445, 0.1431,  ..., 0.3536, 0.3536, 0.3536])\n",
      "tensor([0.4605, 0.1027, 0.2349,  ..., 0.1288, 0.1194, 0.4994])\n",
      "tensor([0.3775, 0.3183, 0.3295,  ..., 0.3760, 0.2679, 0.0273])\n",
      "tensor([0.3934, 0.3466, 0.3540,  ..., 0.3113, 0.1988, 0.4469])\n",
      "tensor([0.6567, 0.1934, 0.0799,  ..., 0.4006, 0.4006, 0.4006])\n",
      "tensor([0.4603, 0.3308, 0.1887,  ..., 0.0643, 0.1874, 0.6515])\n",
      "tensor([0.5492, 0.3476, 0.1312,  ..., 0.2668, 0.2279, 0.1997])\n",
      "tensor([0.3671, 0.3671, 0.3671,  ..., 0.3838, 0.0870, 0.1049])\n",
      "tensor([0.4040, 0.2362, 0.4040,  ..., 0.3536, 0.3536, 0.3536])\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        print(X_batch[0])\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  1],\n",
       "       [ 5, 17]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
